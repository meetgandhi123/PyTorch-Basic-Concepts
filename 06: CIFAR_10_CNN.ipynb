{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-10 CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3AUupWjFWSUuiT6Da5hMa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meetgandhi123/PyTorch-Basic-Concepts/blob/main/CIFAR_10_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGN4xBltD0cA",
        "outputId": "d55fabf1-79d6-4d73-b7de-4dcbf0ed65b6"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transform\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device Config\n",
        "Device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device usage: {Device}')\n",
        "\n",
        "# Hyper Parameters.\n",
        "num_epoch = 20\n",
        "batch_size = 4 \n",
        "learning_rate = 0.001 \n",
        "\n",
        "# Applying Transforms\n",
        "transform = transform.Compose([transform.ToTensor(), \n",
        "                               transform.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
        "                              ])\n",
        "\n",
        "# Dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download = True, transform=transform) \n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download = False, transform=transform)\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle = False)\n",
        "\n",
        "# Classes\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'forg' 'horse', 'ship', 'truck' )\n",
        "\n",
        "# Model\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1,16*5*5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = ConvNet().to(Device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epoch):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 2000 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epoch}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finish Training.')\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(Device)\n",
        "        labels = labels.to(Device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(batch_size):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Accuracy of the network: {acc} %')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device usage: cuda\n",
            "Files already downloaded and verified\n",
            "Epoch [1/20], Step [2000/12500], Loss: 2.3244\n",
            "Epoch [1/20], Step [4000/12500], Loss: 2.3123\n",
            "Epoch [1/20], Step [6000/12500], Loss: 2.2848\n",
            "Epoch [1/20], Step [8000/12500], Loss: 2.3098\n",
            "Epoch [1/20], Step [10000/12500], Loss: 2.3131\n",
            "Epoch [1/20], Step [12000/12500], Loss: 2.2060\n",
            "Epoch [2/20], Step [2000/12500], Loss: 2.2076\n",
            "Epoch [2/20], Step [4000/12500], Loss: 3.8316\n",
            "Epoch [2/20], Step [6000/12500], Loss: 2.1280\n",
            "Epoch [2/20], Step [8000/12500], Loss: 1.5550\n",
            "Epoch [2/20], Step [10000/12500], Loss: 1.6399\n",
            "Epoch [2/20], Step [12000/12500], Loss: 2.3553\n",
            "Epoch [3/20], Step [2000/12500], Loss: 2.1220\n",
            "Epoch [3/20], Step [4000/12500], Loss: 2.1125\n",
            "Epoch [3/20], Step [6000/12500], Loss: 2.5035\n",
            "Epoch [3/20], Step [8000/12500], Loss: 1.5516\n",
            "Epoch [3/20], Step [10000/12500], Loss: 1.1157\n",
            "Epoch [3/20], Step [12000/12500], Loss: 1.8685\n",
            "Epoch [4/20], Step [2000/12500], Loss: 1.2142\n",
            "Epoch [4/20], Step [4000/12500], Loss: 1.4171\n",
            "Epoch [4/20], Step [6000/12500], Loss: 1.3391\n",
            "Epoch [4/20], Step [8000/12500], Loss: 1.5852\n",
            "Epoch [4/20], Step [10000/12500], Loss: 1.5373\n",
            "Epoch [4/20], Step [12000/12500], Loss: 1.7372\n",
            "Epoch [5/20], Step [2000/12500], Loss: 1.5415\n",
            "Epoch [5/20], Step [4000/12500], Loss: 1.2647\n",
            "Epoch [5/20], Step [6000/12500], Loss: 1.6356\n",
            "Epoch [5/20], Step [8000/12500], Loss: 2.0069\n",
            "Epoch [5/20], Step [10000/12500], Loss: 1.2776\n",
            "Epoch [5/20], Step [12000/12500], Loss: 1.5904\n",
            "Epoch [6/20], Step [2000/12500], Loss: 0.4725\n",
            "Epoch [6/20], Step [4000/12500], Loss: 1.5818\n",
            "Epoch [6/20], Step [6000/12500], Loss: 0.9004\n",
            "Epoch [6/20], Step [8000/12500], Loss: 1.1087\n",
            "Epoch [6/20], Step [10000/12500], Loss: 1.3791\n",
            "Epoch [6/20], Step [12000/12500], Loss: 2.5351\n",
            "Epoch [7/20], Step [2000/12500], Loss: 0.9759\n",
            "Epoch [7/20], Step [4000/12500], Loss: 1.3782\n",
            "Epoch [7/20], Step [6000/12500], Loss: 0.4764\n",
            "Epoch [7/20], Step [8000/12500], Loss: 1.5195\n",
            "Epoch [7/20], Step [10000/12500], Loss: 1.4217\n",
            "Epoch [7/20], Step [12000/12500], Loss: 1.6803\n",
            "Epoch [8/20], Step [2000/12500], Loss: 1.2812\n",
            "Epoch [8/20], Step [4000/12500], Loss: 1.0689\n",
            "Epoch [8/20], Step [6000/12500], Loss: 0.2958\n",
            "Epoch [8/20], Step [8000/12500], Loss: 0.7052\n",
            "Epoch [8/20], Step [10000/12500], Loss: 1.3770\n",
            "Epoch [8/20], Step [12000/12500], Loss: 1.9879\n",
            "Epoch [9/20], Step [2000/12500], Loss: 1.0046\n",
            "Epoch [9/20], Step [4000/12500], Loss: 0.4917\n",
            "Epoch [9/20], Step [6000/12500], Loss: 1.6513\n",
            "Epoch [9/20], Step [8000/12500], Loss: 0.4400\n",
            "Epoch [9/20], Step [10000/12500], Loss: 0.5800\n",
            "Epoch [9/20], Step [12000/12500], Loss: 0.8053\n",
            "Epoch [10/20], Step [2000/12500], Loss: 0.9646\n",
            "Epoch [10/20], Step [4000/12500], Loss: 1.1810\n",
            "Epoch [10/20], Step [6000/12500], Loss: 1.1490\n",
            "Epoch [10/20], Step [8000/12500], Loss: 1.1249\n",
            "Epoch [10/20], Step [10000/12500], Loss: 0.8753\n",
            "Epoch [10/20], Step [12000/12500], Loss: 0.8754\n",
            "Epoch [11/20], Step [2000/12500], Loss: 0.9835\n",
            "Epoch [11/20], Step [4000/12500], Loss: 1.8027\n",
            "Epoch [11/20], Step [6000/12500], Loss: 0.6311\n",
            "Epoch [11/20], Step [8000/12500], Loss: 0.4700\n",
            "Epoch [11/20], Step [10000/12500], Loss: 1.1379\n",
            "Epoch [11/20], Step [12000/12500], Loss: 1.0842\n",
            "Epoch [12/20], Step [2000/12500], Loss: 0.6074\n",
            "Epoch [12/20], Step [4000/12500], Loss: 1.3720\n",
            "Epoch [12/20], Step [6000/12500], Loss: 0.9568\n",
            "Epoch [12/20], Step [8000/12500], Loss: 0.8218\n",
            "Epoch [12/20], Step [10000/12500], Loss: 1.1585\n",
            "Epoch [12/20], Step [12000/12500], Loss: 1.1225\n",
            "Epoch [13/20], Step [2000/12500], Loss: 0.6283\n",
            "Epoch [13/20], Step [4000/12500], Loss: 1.8928\n",
            "Epoch [13/20], Step [6000/12500], Loss: 1.2161\n",
            "Epoch [13/20], Step [8000/12500], Loss: 0.8323\n",
            "Epoch [13/20], Step [10000/12500], Loss: 1.0208\n",
            "Epoch [13/20], Step [12000/12500], Loss: 1.2658\n",
            "Epoch [14/20], Step [2000/12500], Loss: 1.1745\n",
            "Epoch [14/20], Step [4000/12500], Loss: 0.6263\n",
            "Epoch [14/20], Step [6000/12500], Loss: 0.5833\n",
            "Epoch [14/20], Step [8000/12500], Loss: 1.3303\n",
            "Epoch [14/20], Step [10000/12500], Loss: 1.0617\n",
            "Epoch [14/20], Step [12000/12500], Loss: 1.1116\n",
            "Epoch [15/20], Step [2000/12500], Loss: 0.7069\n",
            "Epoch [15/20], Step [4000/12500], Loss: 0.2161\n",
            "Epoch [15/20], Step [6000/12500], Loss: 0.7489\n",
            "Epoch [15/20], Step [8000/12500], Loss: 1.5884\n",
            "Epoch [15/20], Step [10000/12500], Loss: 1.0402\n",
            "Epoch [15/20], Step [12000/12500], Loss: 1.6899\n",
            "Epoch [16/20], Step [2000/12500], Loss: 1.3883\n",
            "Epoch [16/20], Step [4000/12500], Loss: 2.2395\n",
            "Epoch [16/20], Step [6000/12500], Loss: 0.8992\n",
            "Epoch [16/20], Step [8000/12500], Loss: 0.7500\n",
            "Epoch [16/20], Step [10000/12500], Loss: 0.8200\n",
            "Epoch [16/20], Step [12000/12500], Loss: 1.0164\n",
            "Epoch [17/20], Step [2000/12500], Loss: 1.1511\n",
            "Epoch [17/20], Step [4000/12500], Loss: 1.0501\n",
            "Epoch [17/20], Step [6000/12500], Loss: 1.6592\n",
            "Epoch [17/20], Step [8000/12500], Loss: 0.6424\n",
            "Epoch [17/20], Step [10000/12500], Loss: 1.9457\n",
            "Epoch [17/20], Step [12000/12500], Loss: 1.4970\n",
            "Epoch [18/20], Step [2000/12500], Loss: 0.5438\n",
            "Epoch [18/20], Step [4000/12500], Loss: 0.1577\n",
            "Epoch [18/20], Step [6000/12500], Loss: 0.6487\n",
            "Epoch [18/20], Step [8000/12500], Loss: 0.8254\n",
            "Epoch [18/20], Step [10000/12500], Loss: 0.7024\n",
            "Epoch [18/20], Step [12000/12500], Loss: 1.5793\n",
            "Epoch [19/20], Step [2000/12500], Loss: 1.7036\n",
            "Epoch [19/20], Step [4000/12500], Loss: 0.9926\n",
            "Epoch [19/20], Step [6000/12500], Loss: 0.2315\n",
            "Epoch [19/20], Step [8000/12500], Loss: 2.4652\n",
            "Epoch [19/20], Step [10000/12500], Loss: 0.8054\n",
            "Epoch [19/20], Step [12000/12500], Loss: 0.9105\n",
            "Epoch [20/20], Step [2000/12500], Loss: 0.8173\n",
            "Epoch [20/20], Step [4000/12500], Loss: 1.1841\n",
            "Epoch [20/20], Step [6000/12500], Loss: 1.1202\n",
            "Epoch [20/20], Step [8000/12500], Loss: 0.7159\n",
            "Epoch [20/20], Step [10000/12500], Loss: 1.3924\n",
            "Epoch [20/20], Step [12000/12500], Loss: 2.1168\n",
            "Finish Training.\n",
            "Accuracy of the network: 63.8 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
